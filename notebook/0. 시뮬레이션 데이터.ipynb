{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:11:17.321955Z",
     "start_time": "2023-01-25T09:11:17.289189Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "units = range(1, 500 + 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = (\n",
    "    pd.DataFrame(\n",
    "        dict(\n",
    "            store=np.repeat(units, 4),\n",
    "            unit_fe=np.repeat(np.random.normal(0, 5, size=len(units)), 4),\n",
    "            weeks_to_xmas=np.tile([3, 2, 1, 0], len(units)),\n",
    "            avg_week_sales=np.repeat(np.random.gamma(20, 1, size=len(units)), 4).round(\n",
    "                2\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        is_on_sale=lambda d: (\n",
    "            (d[\"unit_fe\"] > 0) * np.random.binomial(1, 0.2, d.shape[0])\n",
    "            | (d[\"avg_week_sales\"] > np.random.gamma(25, 1, size=len(units) * 4))\n",
    "            | (np.random.normal(d[\"weeks_to_xmas\"], 2) > 2)\n",
    "            * np.random.binomial(1, 0.7, size=d.shape[0])  # xmas\n",
    "        )\n",
    "        * 1,\n",
    "    )\n",
    "    .assign(\n",
    "        y0=lambda d: (\n",
    "            -10\n",
    "            + 10 * d[\"unit_fe\"]\n",
    "            + d[\"avg_week_sales\"]\n",
    "            + 2 * d[\"avg_week_sales\"] * d[\"weeks_to_xmas\"]\n",
    "        )\n",
    "    )\n",
    "    .assign(y1=lambda d: d[\"y0\"] + 50)\n",
    "    .assign(\n",
    "        tau=lambda d: d[\"y1\"] - d[\"y0\"],\n",
    "        weekly_amount_sold=lambda d: np.random.normal(\n",
    "            np.where(d[\"is_on_sale\"] == 1, d[\"y1\"], d[\"y0\"]), 10\n",
    "        )\n",
    "        .clip(0, np.inf)\n",
    "        .round(2),\n",
    "    )\n",
    "    .drop(columns=[\"unit_fe\", \"y0\", \"y1\", \"tau\"])\n",
    ")\n",
    "\n",
    "data.to_csv(\"../data/xmas_sales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:58:50.923765Z",
     "start_time": "2023-01-24T19:58:50.362564Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "\n",
    "np.random.seed(123)\n",
    "data = (\n",
    "    pd.read_csv(\"../data/online_classroom.csv\")\n",
    "    .assign(\n",
    "        cross_sell_email=lambda d: np.select(\n",
    "            [d[\"format_ol\"].astype(bool), d[\"format_blended\"].astype(bool)],\n",
    "            [\"no_email\", \"long\"],\n",
    "            default=\"short\",\n",
    "        )\n",
    "    )\n",
    "    .assign(age=lambda d: np.random.gamma(1.5, 5, d.shape[0]).astype(int) + 14)\n",
    "    .assign(conversion=lambda d: ((d[\"falsexam\"] - d[\"age\"]) > 70).astype(int))\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"asian\",\n",
    "            \"black\",\n",
    "            \"hawaiian\",\n",
    "            \"hispanic\",\n",
    "            \"unknown\",\n",
    "            \"white\",\n",
    "            \"format_ol\",\n",
    "            \"format_blended\",\n",
    "            \"falsexam\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "data.to_csv(\"../data/cross_sell_email.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:58:50.937159Z",
     "start_time": "2023-01-24T19:58:50.925222Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data = (\n",
    "    pd.read_csv(\"../data/online_classroom.csv\")\n",
    "    .assign(\n",
    "        recommender=lambda d: np.select(\n",
    "            [d[\"format_ol\"].astype(bool), d[\"format_blended\"].astype(bool)],\n",
    "            [\"benchmark\", \"benchmark\"],\n",
    "            default=\"challenger\",\n",
    "        )\n",
    "    )\n",
    "    .assign(age=lambda d: np.random.gamma(1.5, 5, d.shape[0]).astype(int) + 14)\n",
    "    .assign(tenure=lambda d: np.random.gamma(1, 1.1, d.shape[0]).astype(int).clip(0, 4))\n",
    "    .assign(\n",
    "        watch_time=lambda d: np.random.normal(\n",
    "            3 * (d[\"recommender\"] == \"challenger\")\n",
    "            + 10 * d[\"tenure\"]\n",
    "            - 0.8 * d[\"age\"]\n",
    "            + 0.001 * d[\"age\"] ** 2,\n",
    "            11,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        watch_time=lambda d: (\n",
    "            (d[\"watch_time\"] - d[\"watch_time\"].min())\n",
    "            / (d[\"watch_time\"].max() + 20 - d[\"watch_time\"].min())\n",
    "            * 6\n",
    "        ).round(2)\n",
    "    )\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"asian\",\n",
    "            \"gender\",\n",
    "            \"black\",\n",
    "            \"hawaiian\",\n",
    "            \"hispanic\",\n",
    "            \"unknown\",\n",
    "            \"white\",\n",
    "            \"format_ol\",\n",
    "            \"format_blended\",\n",
    "            \"falsexam\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "data.to_csv(\"../data/rec_ab_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:58:51.452368Z",
     "start_time": "2023-01-24T19:58:50.940799Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "risk_data = (\n",
    "    pd.read_csv(\"../data/wage.csv\")\n",
    "    .sample(50000, replace=True)\n",
    "    .assign(wage=lambda d: np.random.normal(100 + d[\"wage\"], 50).round(-1))\n",
    "    .assign(\n",
    "        credit_score1=lambda d: np.random.normal(\n",
    "            100 + 0.1 * d[\"wage\"] + d[\"urban\"] + d[\"educ\"] + 2 * d[\"exper\"], 50\n",
    "        ).round(-1)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score2=lambda d: np.random.normal(\n",
    "            100 + 0.05 * d[\"wage\"] + d[\"hours\"] + d[\"age\"] + 2 * d[\"IQ\"], 50\n",
    "        ).round(-1)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score1=lambda d: np.round(\n",
    "            1000\n",
    "            * (d[\"credit_score1\"] - d[\"credit_score1\"].min() + 20)\n",
    "            / (d[\"credit_score1\"].max() - d[\"credit_score1\"].min()),\n",
    "            0,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score2=lambda d: np.round(\n",
    "            1000\n",
    "            * (d[\"credit_score2\"] - d[\"credit_score2\"].min() + 20)\n",
    "            / (d[\"credit_score2\"].max() - d[\"credit_score2\"].min()),\n",
    "            0,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        credit_limit=lambda d: np.random.normal(\n",
    "            500\n",
    "            + 1.5 * pd.IntervalIndex(pd.qcut(d[\"credit_score1\"], 10)).mid\n",
    "            + 1.5 * pd.IntervalIndex(pd.qcut(d[\"wage\"], 10)).mid,\n",
    "            1000,\n",
    "        )\n",
    "        .round(-2)\n",
    "        .clip(200, np.inf)\n",
    "    )\n",
    "    .assign(\n",
    "        default=lambda d: (\n",
    "            np.random.normal(\n",
    "                +0\n",
    "                - 0.1 * d[\"credit_score1\"]\n",
    "                - 0.5 * d[\"credit_score2\"]\n",
    "                + 0.05 * d[\"IQ\"]\n",
    "                + 0.005 * d[\"credit_limit\"]\n",
    "                - 0.17 * d[\"wage\"],\n",
    "                375,\n",
    "            )\n",
    "            > -50\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"IQ\",\n",
    "            \"lhwage\",\n",
    "            \"tenure\",\n",
    "            \"black\",\n",
    "            \"hours\",\n",
    "            \"sibs\",\n",
    "            \"south\",\n",
    "            \"urban\",\n",
    "            \"brthord\",\n",
    "            \"meduc\",\n",
    "            \"feduc\",\n",
    "            \"age\",\n",
    "        ]\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "spend_data = risk_data.assign(\n",
    "    spend=lambda d: (\n",
    "        np.random.normal(\n",
    "            50\n",
    "            + 120 * np.power(d[\"credit_limit\"], 1 / 2.5)\n",
    "            - d[\"credit_score1\"]\n",
    "            + 1.2 * d[\"wage\"]\n",
    "            - 10 * d[\"exper\"]\n",
    "            - 10 * d[\"educ\"]\n",
    "            + 400 * d[\"married\"]\n",
    "        ).astype(int)\n",
    "    )\n",
    ").drop(columns=[\"default\"])\n",
    "\n",
    "risk_data.to_csv(\"../data/risk_data.csv\", index=False)\n",
    "spend_data.to_csv(\"../data/spend_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:58:51.509074Z",
     "start_time": "2023-01-24T19:58:51.453916Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "risk_data_rnd = (\n",
    "    pd.read_csv(\"../data/wage.csv\")\n",
    "    .sample(10000, replace=True)\n",
    "    .assign(wage=lambda d: np.random.normal(100 + d[\"wage\"], 50).round(-1))\n",
    "    .assign(\n",
    "        credit_score1=lambda d: np.random.normal(\n",
    "            100 + 0.1 * d[\"wage\"] + d[\"urban\"] + d[\"educ\"] + 2 * d[\"exper\"], 50\n",
    "        ).round(-1)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score2=lambda d: np.random.normal(\n",
    "            100 + 0.05 * d[\"wage\"] + d[\"hours\"] + d[\"age\"] + 2 * d[\"IQ\"], 50\n",
    "        ).round(-1)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score1=lambda d: np.round(\n",
    "            1000\n",
    "            * (d[\"credit_score1\"] - d[\"credit_score1\"].min() + 20)\n",
    "            / (d[\"credit_score1\"].max() - d[\"credit_score1\"].min()),\n",
    "            0,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score2=lambda d: np.round(\n",
    "            1000\n",
    "            * (d[\"credit_score2\"] - d[\"credit_score2\"].min() + 20)\n",
    "            / (d[\"credit_score2\"].max() - d[\"credit_score2\"].min()),\n",
    "            0,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score1_buckets=lambda d: (\n",
    "            (d[\"credit_score1\"] / 200).round(0) * 200\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_limit=lambda d: (\n",
    "            np.random.beta(1 + d[\"credit_score1_buckets\"] / 100, 6) * 10000\n",
    "        ).round(-2)\n",
    "    )\n",
    "    .assign(\n",
    "        default=lambda d: (\n",
    "            np.random.normal(\n",
    "                +0\n",
    "                - 0.1 * d[\"credit_score1\"]\n",
    "                - 0.5 * d[\"credit_score2\"]\n",
    "                + 0.05 * d[\"IQ\"]\n",
    "                + 0.5 * d[\"sibs\"]\n",
    "                - 0.5 * d[\"feduc\"]\n",
    "                + 0.005 * d[\"credit_limit\"]\n",
    "                - 0.17 * d[\"wage\"],\n",
    "                100,\n",
    "            )\n",
    "            > -300\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"IQ\",\n",
    "            \"lhwage\",\n",
    "            \"tenure\",\n",
    "            \"black\",\n",
    "            \"hours\",\n",
    "            \"sibs\",\n",
    "            \"south\",\n",
    "            \"urban\",\n",
    "            \"brthord\",\n",
    "            \"meduc\",\n",
    "            \"feduc\",\n",
    "            \"age\",\n",
    "        ]\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "risk_data_rnd.to_csv(\"../data/risk_data_rnd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:16:08.938041Z",
     "start_time": "2023-01-25T09:16:08.903066Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "spend_data_rnd = (\n",
    "    pd.read_csv(\"../data/wage.csv\")\n",
    "    .sample(500, replace=True)\n",
    "    .assign(wage=lambda d: np.random.normal(100 + d[\"wage\"], 50).round(-1))\n",
    "    .assign(credit_score1=lambda d: np.random.normal(100, 50, len(d)).round(-1))\n",
    "    .assign(\n",
    "        credit_score1=lambda d: np.round(\n",
    "            1000\n",
    "            * (d[\"credit_score1\"] - d[\"credit_score1\"].min() + 20)\n",
    "            / (d[\"credit_score1\"].max() - d[\"credit_score1\"].min()),\n",
    "            0,\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        credit_score1_buckets=lambda d: (\n",
    "            (d[\"credit_score1\"] / 200).round(0) * 200\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .assign(\n",
    "        credit_limit=lambda d: (\n",
    "            np.random.beta(1 + d[\"credit_score1_buckets\"] / 100, 6) * 10000\n",
    "        ).round(-2)\n",
    "    )\n",
    "    .assign(\n",
    "        spend=lambda d: (\n",
    "            np.random.normal(\n",
    "                500\n",
    "                + 20 * np.power(d[\"credit_limit\"], 1 / 2)\n",
    "                + 1.2 * d[\"wage\"]\n",
    "                - 10 * d[\"exper\"]\n",
    "                - 10 * d[\"educ\"]\n",
    "                + 400 * d[\"married\"],\n",
    "                600,\n",
    "            ).astype(int)\n",
    "        )\n",
    "    )\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"IQ\",\n",
    "            \"lhwage\",\n",
    "            \"tenure\",\n",
    "            \"black\",\n",
    "            \"hours\",\n",
    "            \"sibs\",\n",
    "            \"south\",\n",
    "            \"urban\",\n",
    "            \"brthord\",\n",
    "            \"meduc\",\n",
    "            \"feduc\",\n",
    "            \"age\",\n",
    "        ]\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "spend_data_rnd.to_csv(\"../data/spend_data_rnd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T18:04:52.239145Z",
     "start_time": "2023-03-12T18:04:49.773194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46876/2760081185.py:14: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  .assign(department_size = lambda d: d.groupby(\"departament_id\")[\"n_of_reports\"].transform(sum))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "df = (\n",
    "    pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/master/causal-inference-for-the-brave-and-true/data/learning_mindset.csv\"\n",
    "    )\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"schoolid\": \"departament_id\",\n",
    "            \"achievement_score\": \"engagement_score\",\n",
    "            \"success_expect\": \"tenure\",\n",
    "            \"school_achievement\": \"last_engagement_score\",\n",
    "            \"school_urbanicity\": \"role\",\n",
    "            \"school_poverty\": \"department_score\",\n",
    "            \"school_size\": \"department_size\",\n",
    "            \"ethnicity\": \"n_of_reports\",\n",
    "        }\n",
    "    )\n",
    "    # reduce overlapp for better examples\n",
    "    .assign(\n",
    "        intervention=lambda d: (\n",
    "            d[\"intervention\"].astype(bool)\n",
    "            | (np.random.normal(d[\"last_engagement_score\"]) > 1.65)\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .assign(\n",
    "        intervention=lambda d: (\n",
    "            d[\"intervention\"].astype(bool) | (np.random.normal(d[\"tenure\"], 2) > 7)\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .assign(n_of_reports=lambda d: d[\"n_of_reports\"].clip(0, 8))\n",
    "    .assign(\n",
    "        department_size=lambda d: d.groupby(\"departament_id\")[\"n_of_reports\"].transform(\n",
    "            sum\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        last_engagement_score=lambda d: np.random.normal(d[\"last_engagement_score\"])\n",
    "    )\n",
    "    .drop(columns=[\"frst_in_family\", \"school_mindset\", \"school_ethnic_minority\"])\n",
    ")\n",
    "\n",
    "\n",
    "df.to_csv(\"../data/management_training.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T18:04:52.296332Z",
     "start_time": "2023-03-12T18:04:52.248495Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "n = 10000\n",
    "\n",
    "x1 = np.random.uniform(-1, 1, n)\n",
    "x2 = np.random.uniform(-1, 1, n)\n",
    "t = np.random.normal(7.5 - 1 * (x1 + x2), 2).clip(1, 14).round(1)\n",
    "y = np.random.normal((23 - 0.8 * t - 8 * (x1 + x2))).clip(1, np.inf).round(0)\n",
    "\n",
    "df_cont_t = pd.DataFrame(dict(ml_1=x1, ml_2=x2, interest=t, duration=y))\n",
    "\n",
    "df_cont_t.to_csv(\"../data/interest_rate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T13:24:24.810974Z",
     "start_time": "2023-06-26T13:24:24.700439Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries import holiday\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "month_fe = {\n",
    "    1: 23,\n",
    "    2: 11,\n",
    "    3: 10,\n",
    "    4: 9,\n",
    "    5: 4,\n",
    "    6: 2,\n",
    "    7: 10,\n",
    "    8: 5,\n",
    "    9: 10,\n",
    "    10: 20,\n",
    "    11: 25,\n",
    "    12: 30,\n",
    "}\n",
    "\n",
    "week_fe = {0: 2, 1: 2, 2: 3, 3: 7, 4: 15, 5: 12, 6: 5}\n",
    "\n",
    "store_fe = {0: 20, 1: 14, 2: 3, 3: 10, 4: 9, 5: 12, 6: 5}\n",
    "\n",
    "day = pd.Series(pd.date_range(\"2016-01-01\", \"2019-01-01\"))\n",
    "weekend = day.dt.weekday >= 5\n",
    "is_holiday = day.isin(\n",
    "    holiday.USFederalHolidayCalendar().holidays(\"2016-01-01\", \"2019-01-01\")\n",
    ")\n",
    "is_dec = day.dt.month == 12\n",
    "is_nov = day.dt.month == 11\n",
    "\n",
    "time_data = pd.DataFrame(\n",
    "    dict(\n",
    "        day=day,\n",
    "        month=day.dt.month,\n",
    "        weekday=day.dt.weekday,\n",
    "        weekend=weekend,\n",
    "        is_holiday=is_holiday,\n",
    "        is_dec=day.dt.month == 12,\n",
    "        is_nov=day.dt.month == 11,\n",
    "    )\n",
    ").assign(\n",
    "    month_fe=lambda d: d[\"month\"].map(month_fe),\n",
    "    week_fe=lambda d: d[\"weekday\"].map(week_fe),\n",
    ")\n",
    "\n",
    "data = (\n",
    "    pd.concat([time_data.assign(rest_id=i) for i in range(len(store_fe))])\n",
    "    .assign(store_fe=lambda d: d[\"rest_id\"].map(store_fe))\n",
    "    .assign(\n",
    "        competitors_price=lambda d: (\n",
    "            np.random.beta(\n",
    "                5 + d[\"is_holiday\"] + d[\"is_dec\"] * 2 + d[\"is_nov\"],\n",
    "                0.5 * d[\"store_fe\"],\n",
    "                len(d),\n",
    "            )\n",
    "            * 9\n",
    "            + 1\n",
    "        ).round(2)\n",
    "    )\n",
    "    .assign(\n",
    "        sales0=lambda d: np.random.poisson(\n",
    "            d[\"month_fe\"]\n",
    "            + d[\"store_fe\"]\n",
    "            + d[\"week_fe\"]\n",
    "            + 3 * d[\"competitors_price\"]\n",
    "            + 10 * d[\"is_holiday\"]\n",
    "            + 5 * d[\"weekend\"]\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        effect=lambda d: np.random.poisson(\n",
    "            150\n",
    "            + d[\"month_fe\"]\n",
    "            - d[\"store_fe\"]\n",
    "            + d[\"week_fe\"]\n",
    "            - 40 * np.sqrt(d[\"competitors_price\"])\n",
    "            + 5 * d[\"is_holiday\"]\n",
    "            - 3 * d[\"weekend\"]\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        discounts=lambda d: ((np.random.beta(1, 3, size=len(d)) * 50) / 5).astype(int)\n",
    "        * 5\n",
    "    )\n",
    "    .assign(sales=lambda d: d[\"sales0\"] + 0.5 * d[\"effect\"] * d[\"discounts\"])[\n",
    "        [\n",
    "            \"rest_id\",\n",
    "            \"day\",\n",
    "            \"month\",\n",
    "            \"weekday\",\n",
    "            \"weekend\",\n",
    "            \"is_holiday\",\n",
    "            \"is_dec\",\n",
    "            \"is_nov\",\n",
    "            \"competitors_price\",\n",
    "            \"discounts\",\n",
    "            \"sales\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "data.to_csv(\"../data/daily_restaurant_sales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T13:24:28.018989Z",
     "start_time": "2023-06-26T13:24:27.911853Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categs = {\n",
    "    \"vehicle\": 0.1,\n",
    "    \"food\": 1,\n",
    "    \"beverage\": 1,\n",
    "    \"art\": 1,\n",
    "    \"baby\": 1,\n",
    "    \"personal_care\": 1,\n",
    "    \"toys\": 1,\n",
    "    \"clothing\": 2,\n",
    "    \"decor\": 1,\n",
    "    \"cell_phones\": 3,\n",
    "    \"construction\": 1,\n",
    "    \"home_appliances\": 1,\n",
    "    \"electronics\": 2,\n",
    "    \"sports\": 1,\n",
    "    \"tools\": 1,\n",
    "    \"games\": 2,\n",
    "    \"industry\": 1,\n",
    "    \"pc\": 2,\n",
    "    \"jewel\": 1,\n",
    "    \"books\": 1,\n",
    "    \"music_books_movies\": 1,\n",
    "    \"health\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "n = 10000\n",
    "\n",
    "age = (18 + np.random.beta(2, 7, n) * 90).round(0)\n",
    "tenure = np.random.exponential(0.5, n).round(0)\n",
    "ammount_spent = (\n",
    "    np.random.exponential(1, n) * 100\n",
    "    + np.random.binomial(1, 0.01, n) * np.random.uniform(500, 50000, n)\n",
    ").round(2)\n",
    "\n",
    "categ_purchage = {cat: np.random.poisson(l, n) for cat, l in categs.items()}\n",
    "\n",
    "X = pd.DataFrame(categ_purchage).assign(\n",
    "    age=age,\n",
    "    tenure=tenure,\n",
    "    ammount_spent=ammount_spent,\n",
    ")\n",
    "\n",
    "mkt_email_rnd = np.random.binomial(1, 0.5, n)\n",
    "\n",
    "coefs = np.concatenate([np.random.uniform(-1, 1, len(categs)), np.array([1, 20, 0.4])])\n",
    "y0 = np.random.exponential(X.values.dot(coefs))\n",
    "\n",
    "cate_coef = np.concatenate(\n",
    "    [np.random.uniform(0, 100, len(categs)), np.array([-1, -20, 0])]\n",
    ")\n",
    "\n",
    "tau = np.random.normal(X.values.dot(cate_coef))\n",
    "\n",
    "data_rnd = X.assign(mkt_email=mkt_email_rnd, next_mnth_pv=y0 + tau * mkt_email_rnd)[\n",
    "    [\"mkt_email\", \"next_mnth_pv\", \"age\", \"tenure\", \"ammount_spent\"]\n",
    "    + list(categs.keys())\n",
    "].round(2)\n",
    "\n",
    "\n",
    "data_rnd.to_csv(\"../data/email_rnd_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T13:24:30.494095Z",
     "start_time": "2023-06-26T13:24:28.482269Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "n = 300000\n",
    "\n",
    "age = (18 + np.random.beta(2, 7, n) * 90).round(0)\n",
    "tenure = np.random.exponential(0.5, n).round(0)\n",
    "ammount_spent = (\n",
    "    np.random.exponential(1, n) * 100\n",
    "    + np.random.binomial(1, 0.01, n) * np.random.uniform(500, 50000, n)\n",
    ").round(2)\n",
    "\n",
    "categ_purchage = {cat: np.random.poisson(l, n) for cat, l in categs.items()}\n",
    "\n",
    "X = pd.DataFrame(categ_purchage).assign(\n",
    "    age=age,\n",
    "    tenure=tenure,\n",
    "    ammount_spent=ammount_spent,\n",
    ")\n",
    "\n",
    "coefs_ps = np.concatenate(\n",
    "    [np.random.uniform(-1, 1, len(categs)), np.array([-1, 40, 0.4])]\n",
    ")\n",
    "\n",
    "\n",
    "mkt_email_biased = (np.random.normal(X.values.dot(coefs_ps), 2000) > 0).astype(int)\n",
    "\n",
    "y0 = np.random.exponential(X.values.dot(coefs))\n",
    "tau = np.random.normal(X.values.dot(cate_coef))\n",
    "\n",
    "data_biased = X.assign(\n",
    "    mkt_email=mkt_email_biased, next_mnth_pv=y0 + tau * mkt_email_biased\n",
    ")[\n",
    "    [\"mkt_email\", \"next_mnth_pv\", \"age\", \"tenure\", \"ammount_spent\"]\n",
    "    + list(categs.keys())\n",
    "].round(2)\n",
    "\n",
    "data_biased.to_csv(\"../data/email_obs_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T13:24:33.155603Z",
     "start_time": "2023-06-26T13:24:31.504219Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.tseries import holiday\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "month_fe = {\n",
    "    1: 23,\n",
    "    2: 11,\n",
    "    3: 10,\n",
    "    4: 9,\n",
    "    5: 4,\n",
    "    6: 2,\n",
    "    7: 10,\n",
    "    8: 5,\n",
    "    9: 10,\n",
    "    10: 20,\n",
    "    11: 25,\n",
    "    12: 30,\n",
    "}\n",
    "\n",
    "week_fe = {0: 2, 1: 2, 2: 3, 3: 7, 4: 15, 5: 12, 6: 5}\n",
    "\n",
    "store_fe = {0: 20, 1: 14, 2: 3, 3: 10, 4: 9, 5: 12, 6: 5}\n",
    "\n",
    "day = pd.Series(pd.date_range(\"2016-01-01\", \"2019-01-01\"))\n",
    "weekend = day.dt.weekday >= 5\n",
    "is_holiday = day.isin(\n",
    "    holiday.USFederalHolidayCalendar().holidays(\"2016-01-01\", \"2019-01-01\")\n",
    ")\n",
    "is_dec = day.dt.month == 12\n",
    "is_nov = day.dt.month == 11\n",
    "\n",
    "time_data = pd.DataFrame(\n",
    "    dict(\n",
    "        day=day,\n",
    "        month=day.dt.month,\n",
    "        weekday=day.dt.weekday,\n",
    "        weekend=weekend,\n",
    "        is_holiday=is_holiday,\n",
    "        is_dec=day.dt.month == 12,\n",
    "        is_nov=day.dt.month == 11,\n",
    "    )\n",
    ").assign(\n",
    "    month_fe=lambda d: d[\"month\"].map(month_fe),\n",
    "    week_fe=lambda d: d[\"weekday\"].map(week_fe),\n",
    ")\n",
    "\n",
    "data_cont = (\n",
    "    pd.concat([time_data.assign(rest_id=i) for i in range(len(store_fe))])\n",
    "    .assign(store_fe=lambda d: d[\"rest_id\"].map(store_fe))\n",
    "    .assign(\n",
    "        competitors_price=lambda d: (\n",
    "            np.random.beta(\n",
    "                5 + d[\"is_holiday\"] + d[\"is_dec\"] * 2 + d[\"is_nov\"],\n",
    "                0.5 * d[\"store_fe\"],\n",
    "                len(d),\n",
    "            )\n",
    "            * 9\n",
    "            + 1\n",
    "        ).round(2)\n",
    "    )\n",
    "    .assign(\n",
    "        sales0=lambda d: np.random.poisson(\n",
    "            d[\"month_fe\"]\n",
    "            + d[\"store_fe\"]\n",
    "            + d[\"week_fe\"]\n",
    "            + 3 * d[\"competitors_price\"]\n",
    "            + 10 * d[\"is_holiday\"]\n",
    "            + 5 * d[\"weekend\"]\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        effect=lambda d: np.random.poisson(\n",
    "            150\n",
    "            + d[\"month_fe\"]\n",
    "            - d[\"store_fe\"]\n",
    "            + d[\"week_fe\"]\n",
    "            - 40 * np.sqrt(d[\"competitors_price\"])\n",
    "            + 5 * d[\"is_holiday\"]\n",
    "            - 3 * d[\"weekend\"]\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        discounts=lambda d: ((np.random.beta(1, 3, size=len(d)) * 50) / 5).astype(int)\n",
    "        * 5\n",
    "    )\n",
    "    .assign(sales=lambda d: d[\"sales0\"] + 0.5 * d[\"effect\"] * d[\"discounts\"])[\n",
    "        [\n",
    "            \"rest_id\",\n",
    "            \"day\",\n",
    "            \"month\",\n",
    "            \"weekday\",\n",
    "            \"weekend\",\n",
    "            \"is_holiday\",\n",
    "            \"is_dec\",\n",
    "            \"is_nov\",\n",
    "            \"competitors_price\",\n",
    "            \"discounts\",\n",
    "            \"sales\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "data_cont.to_csv(\"../data/discount_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T13:24:36.557346Z",
     "start_time": "2023-06-26T13:24:36.438111Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 생성\n",
    "date = pd.date_range(\"2021-05-01\", \"2021-07-31\", freq=\"D\")\n",
    "cohorts = pd.to_datetime([\"2021-05-15\", \"2021-06-04\", \"2021-06-20\"])\n",
    "poss_regions = [\"S\", \"N\", \"W\", \"E\"]\n",
    "\n",
    "reg_ps = dict(zip(poss_regions, [0.3, 0.6, 0.7, 0.8]))\n",
    "reg_fe = dict(zip(poss_regions, [20, 16, 8, 2]))\n",
    "reg_trend = dict(zip(poss_regions, [0, 0.2, 0.4, 0.6]))\n",
    "\n",
    "units = np.array(range(1, 200 + 1))\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "unit_reg = np.random.choice(poss_regions, len(units))\n",
    "exp_trend = np.random.exponential(0.01, len(units))\n",
    "treated_unit = np.random.binomial(1, np.vectorize(reg_ps.__getitem__)(unit_reg))\n",
    "\n",
    "# staggered adoption dgp\n",
    "df = (\n",
    "    pd.DataFrame(\n",
    "        dict(\n",
    "            date=np.tile(date, len(units)),  # 수정: date.date 대신 date 사용\n",
    "            city=np.repeat(units, len(date)),\n",
    "            region=np.repeat(unit_reg, len(date)),\n",
    "            treated_unit=np.repeat(treated_unit, len(date)),\n",
    "            cohort=np.repeat(np.random.choice(cohorts.date, len(units)), len(date)),\n",
    "            eff_heter=np.repeat(np.random.exponential(1, size=len(units)), len(date)),\n",
    "            unit_fe=np.repeat(np.random.normal(0, 2, size=len(units)), len(date)),\n",
    "            time_fe=np.tile(np.random.normal(size=len(date)), len(units)),\n",
    "            week_day=np.tile(date.weekday, len(units)),\n",
    "            w_seas=np.tile(abs(5 - date.weekday) % 7, len(units)),\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        reg_fe=lambda d: d[\"region\"].map(reg_fe),\n",
    "        reg_trend=lambda d: d[\"region\"].map(reg_trend),\n",
    "        reg_ps=lambda d: d[\"region\"].map(reg_ps),\n",
    "        trend=lambda d: (\n",
    "            pd.to_datetime(d[\"date\"]) - pd.to_datetime(d[\"date\"]).min()\n",
    "        ).dt.days,\n",
    "        day=lambda d: (\n",
    "            pd.to_datetime(d[\"date\"]) - pd.to_datetime(d[\"date\"]).min()\n",
    "        ).dt.days,\n",
    "        cohort=lambda d: np.where(\n",
    "            d[\"treated_unit\"] == 1, d[\"cohort\"], pd.to_datetime(\"2100-01-01\")\n",
    "        ),\n",
    "    )\n",
    "    .assign(\n",
    "        treated=lambda d: (\n",
    "            (d[\"date\"] >= d[\"cohort\"]) & (d[\"treated_unit\"] == 1)\n",
    "        ).astype(int),\n",
    "    )\n",
    "    .assign(\n",
    "        y0=lambda d: np.round(\n",
    "            10\n",
    "            + d[\"treated_unit\"]\n",
    "            + d[\"reg_trend\"] * d[\"trend\"] / 2\n",
    "            + d[\"unit_fe\"]\n",
    "            + 0.4 * d[\"time_fe\"]\n",
    "            + 2 * d[\"reg_fe\"]\n",
    "            + d[\"w_seas\"] / 5,\n",
    "            0,\n",
    "        ),\n",
    "    )\n",
    "    .assign(\n",
    "        #     y0 = lambda d: np.round(d[\"y0\"] + d.groupby(\"city\")[\"y0\"].shift(1).fillna(0)*0.2, 0)\n",
    "    )\n",
    "    .assign(\n",
    "        y1=lambda d: d[\"y0\"]\n",
    "        + np.minimum(\n",
    "            0.2\n",
    "            * (\n",
    "                np.maximum(\n",
    "                    0, (pd.to_datetime(d[\"date\"]) - pd.to_datetime(d[\"cohort\"])).dt.days\n",
    "                )\n",
    "            ),\n",
    "            1,\n",
    "        )\n",
    "        * d[\"eff_heter\"]\n",
    "        * 2\n",
    "    )\n",
    "    .assign(\n",
    "        tau=lambda d: d[\"y1\"] - d[\"y0\"],\n",
    "        downloads=lambda d: np.where(d[\"treated\"] == 1, d[\"y1\"], d[\"y0\"])\n",
    "        + np.random.normal(0, 0.7, len(d)),\n",
    "        #     date = lambda d: pd.to_datetime(d[\"date\"]),\n",
    "    )\n",
    "    .round({\"downloads\": 0})\n",
    ")\n",
    "\n",
    "# 필터링 및 데이터 저장\n",
    "reg_filter = [\"N\", \"S\", \"E\", \"W\"]\n",
    "\n",
    "mkt_data_all = (\n",
    "    df.query(\"region.isin(@reg_filter)\")\n",
    "    .query(\"date.astype('string') <= '2021-06-01'\")\n",
    "    #                 .assign(cohort = lambda d: np.where(d[\"cohort\"].astype(str) <= \"2021-06-01\", d[\"cohort\"], \"2050-01-01\"))\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"reg_fe\",\n",
    "            \"time_fe\",\n",
    "            \"cohort\",\n",
    "            \"w_seas\",\n",
    "            \"week_day\",\n",
    "            \"reg_trend\",\n",
    "            \"trend\",\n",
    "            \"day\",\n",
    "            \"unit_fe\",\n",
    "            \"y0\",\n",
    "            \"y1\",\n",
    "            \"eff_heter\",\n",
    "            \"reg_ps\",\n",
    "            \"treated_unit\",\n",
    "        ]\n",
    "    )\n",
    "    .assign(post=lambda d: (d[\"date\"].astype(str) >= \"2021-05-15\").astype(int))\n",
    "    .assign(treated=lambda d: d.groupby(\"city\")[\"treated\"].transform(\"max\"))\n",
    ")\n",
    "\n",
    "mkt_data = mkt_data_all.query(\"region=='S'\")\n",
    "\n",
    "mkt_data_cohorts = (\n",
    "    df.assign(post=lambda d: (d[\"date\"] >= d[\"cohort\"]).astype(int))\n",
    "    .assign(treated=lambda d: d.groupby(\"city\")[\"treated\"].transform(\"max\"))\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"reg_fe\",\n",
    "            \"time_fe\",\n",
    "            \"w_seas\",\n",
    "            \"week_day\",\n",
    "            \"reg_trend\",\n",
    "            \"trend\",\n",
    "            \"day\",\n",
    "            \"unit_fe\",\n",
    "            \"y0\",\n",
    "            \"y1\",\n",
    "            \"eff_heter\",\n",
    "            \"reg_ps\",\n",
    "            \"treated_unit\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "mkt_data_cohorts.to_csv(\"../data/offline_mkt_staggered.csv\", index=False)\n",
    "mkt_data_all.to_csv(\"../data/short_offline_mkt_all_regions.csv\", index=False)\n",
    "mkt_data.to_csv(\"../data/short_offline_mkt_south.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.262446Z",
     "start_time": "2023-01-24T19:59:00.189182Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "\n",
    "def simulate_series(\n",
    "    dates,\n",
    "    name,\n",
    "    pop,\n",
    "    pop_pct,\n",
    "    ws_w,\n",
    "    ms_w,\n",
    "    ys_w,\n",
    "    ar_w,\n",
    "    trend_w,\n",
    "    noise_w,\n",
    "    ar=1,\n",
    "    ma=7,\n",
    "):\n",
    "    ws = abs(5 - dates.weekday) % 7\n",
    "    ms = abs(dates.day - 10)\n",
    "    ys = abs(6 - dates.month) % 12\n",
    "\n",
    "    arma = ArmaProcess(ar, ma).generate_sample(len(dates))\n",
    "    trend = np.arange(0, len(dates))\n",
    "    noise = np.random.normal(0, 1, len(dates))\n",
    "\n",
    "    comps = (ws, ms, ys, arma, trend, noise)\n",
    "    comp_noise = np.random.uniform(0.95, 1.05, len(comps))\n",
    "\n",
    "    coefs = (ws_w, ms_w, ys_w, ar_w, trend_w, noise_w)\n",
    "\n",
    "    result = (\n",
    "        sum(c * w * noise for c, w, noise in zip(comps, coefs, comp_noise))\n",
    "        * pop\n",
    "        * pop_pct\n",
    "        * 0.005\n",
    "    )\n",
    "\n",
    "    return pd.Series(result, name=name).clip(0, np.inf).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.271093Z",
     "start_time": "2023-01-24T19:59:00.263454Z"
    }
   },
   "outputs": [],
   "source": [
    "br_cities = pd.read_csv(\"../data/br_cities.csv\")\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "states = br_cities[\"state\"].unique()\n",
    "pop_pct = np.random.beta(5, 10, len(states)) * 0.1\n",
    "trends = np.random.beta(9, 5, len(states)) * 0.5 - 0.25\n",
    "week_sas = np.random.uniform(2, 4, len(states))\n",
    "m_sas = np.random.uniform(0, 1, len(states))\n",
    "y_sas = np.random.uniform(0, 2, len(states))\n",
    "\n",
    "states_params = pd.DataFrame(\n",
    "    dict(\n",
    "        pop_pct=pop_pct,\n",
    "        state=states,\n",
    "        trends=trends,\n",
    "        week_sas=week_sas,\n",
    "        m_sas=m_sas,\n",
    "        y_sas=y_sas,\n",
    "    )\n",
    ")\n",
    "\n",
    "# states_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.279476Z",
     "start_time": "2023-01-24T19:59:00.272275Z"
    }
   },
   "outputs": [],
   "source": [
    "br_cities = pd.read_csv(\"../data/br_cities.csv\").merge(states_params, on=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.456232Z",
     "start_time": "2023-01-24T19:59:00.280636Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "date = pd.date_range(\"2022-03-01\", \"2022-06-30\", freq=\"D\")\n",
    "\n",
    "series = [\n",
    "    simulate_series(\n",
    "        date,\n",
    "        name=city[\"city\"],\n",
    "        pop=city[\"pop\"] * city[\"pop_pct\"],\n",
    "        pop_pct=city[\"pop_pct\"],\n",
    "        ws_w=city[\"week_sas\"],\n",
    "        ms_w=city[\"m_sas\"],\n",
    "        ys_w=city[\"y_sas\"],\n",
    "        ar_w=0.1,\n",
    "        trend_w=city[\"trends\"],\n",
    "        noise_w=1000 / np.sqrt(city[\"pop\"] * city[\"pop_pct\"]),\n",
    "    )\n",
    "    .round(0)\n",
    "    .to_frame()\n",
    "    .assign(\n",
    "        population=city[\"pop\"],\n",
    "        city=city[\"city\"],\n",
    "        state=city[\"state\"],\n",
    "        date=date,\n",
    "    )\n",
    "    .rename(columns={city[\"city\"]: \"app_download\"})\n",
    "    for _, city in br_cities.sort_values(\"pop\", ascending=False).head(50).iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.481510Z",
     "start_time": "2023-01-24T19:59:00.457362Z"
    }
   },
   "outputs": [],
   "source": [
    "features = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                activity=(\n",
    "                    ArmaProcess([1, 0.9, 0.8, 0.7], 7).generate_sample(len(date))\n",
    "                    + row[\"hdi\"]\n",
    "                ).round(0)\n",
    "                * 3,\n",
    "                date=date,\n",
    "                state=row[\"state\"],\n",
    "                city=row[\"city\"],\n",
    "            )\n",
    "        )\n",
    "        for _, row in br_cities.sort_values(\"pop\", ascending=False).head(50).iterrows()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.486234Z",
     "start_time": "2023-01-24T19:59:00.482811Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_effect(df, city_col, date_col, y_col, city, start_at, window, effect_pct):\n",
    "    window_mask = (df[date_col] >= start_at) & (df[city_col] == city)\n",
    "\n",
    "    def rbf(x, center, sigma):\n",
    "        return np.exp(-((x - center) ** 2) / sigma**2)\n",
    "\n",
    "    # 수정됨: window.2 대신 window / 2 사용\n",
    "    kernel = rbf(\n",
    "        (df[date_col] - pd.to_datetime(start_at)).dt.days - window / 2 - 1,\n",
    "        0,\n",
    "        window / 2,\n",
    "    )\n",
    "\n",
    "    y = np.where(window_mask, df[y_col] + df[y_col] * effect_pct * kernel, df[y_col])\n",
    "\n",
    "    return df.assign(**{y_col: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.542421Z",
     "start_time": "2023-01-24T19:59:00.487636Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat(series)\n",
    "\n",
    "df = simulate_effect(\n",
    "    df, \"city\", \"date\", \"app_download\", \"sao_paulo\", \"2022-05-01\", 40, 0.4\n",
    ")\n",
    "df = simulate_effect(\n",
    "    df, \"city\", \"date\", \"app_download\", \"joao_pessoa\", \"2022-05-01\", 40, 0.5\n",
    ")\n",
    "df = simulate_effect(\n",
    "    df, \"city\", \"date\", \"app_download\", \"porto_alegre\", \"2022-05-01\", 40, 0.6\n",
    ")\n",
    "\n",
    "df = df.assign(\n",
    "    post=(df[\"date\"] >= \"2022-05-01\") * 1,\n",
    "    treated=(df[\"city\"].isin([\"sao_paulo\", \"joao_pessoa\", \"porto_alegre\"])) * 1,\n",
    ")\n",
    "\n",
    "df.to_csv(\"../data/online_mkt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.602696Z",
     "start_time": "2023-01-24T19:59:00.543585Z"
    }
   },
   "outputs": [],
   "source": [
    "df_norm = df.assign(app_download_pct=100 * df[\"app_download\"] / df[\"population\"])\n",
    "\n",
    "\n",
    "df_norm_cov = (\n",
    "    df_norm.merge(br_cities[[\"city\", \"state\", \"hdi\"]])\n",
    "    .assign(\n",
    "        comp_download_pct=lambda d: 100\n",
    "        * (0.8 * d[\"app_download\"] + d[\"hdi\"])\n",
    "        / d[\"population\"]\n",
    "    )\n",
    "    .drop(columns=[\"hdi\"])\n",
    ")\n",
    "\n",
    "df_norm_cov.to_csv(\"../data/online_mkt_cov.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:48:17.398154Z",
     "start_time": "2023-02-06T12:48:17.373468Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "T = 120\n",
    "m = 2\n",
    "p = 0.5\n",
    "\n",
    "\n",
    "def y_given_d(d, effect_params=[3, 2, 1], T=T, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    x = np.arange(1, T + 1)\n",
    "    return (\n",
    "        np.log(x + 1)\n",
    "        + 2 * np.sin(x * 2 * np.pi / 24)\n",
    "        + np.convolve(~d.astype(bool), effect_params)[: -(len(effect_params) - 1)]\n",
    "        + np.random.normal(0, 1, T)\n",
    "        #             + ArmaProcess([3,2,], 3).generate_sample(T)\n",
    "    ).round(2)\n",
    "\n",
    "\n",
    "# correct tau = 3+2+1=6\n",
    "effect_params = [3, 2, 1]\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "df_sb_every = (\n",
    "    pd.DataFrame(\n",
    "        dict(\n",
    "            d=np.random.binomial(1, 0.5, T),\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        delivery_time=lambda d: y_given_d(d[\"d\"], seed=1),\n",
    "        delivery_time_1=lambda d: y_given_d(np.ones(T), seed=1),\n",
    "        delivery_time_0=lambda d: y_given_d(np.zeros(T), seed=1),\n",
    "    )\n",
    "    .assign(tau=lambda d: d[\"delivery_time_1\"] - d[\"delivery_time_0\"])\n",
    ")\n",
    "\n",
    "\n",
    "df_sb_every.to_csv(\"../data/sb_exp_every.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T12:37:42.355708Z",
     "start_time": "2023-02-06T12:37:42.333596Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_d(rand_points, p):\n",
    "    result = [np.random.binomial(1, p)]\n",
    "\n",
    "    for t in rand_points[1:]:\n",
    "        result.append(np.random.binomial(1, p) * t + (1 - t) * result[-1])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# 가상의 y_given_d 함수 (실제 구현에 따라 수정 필요)\n",
    "def y_given_d(d, T, seed):\n",
    "    np.random.seed(seed)\n",
    "    return np.random.rand(T)  # 예시로 랜덤 배열 반환\n",
    "\n",
    "\n",
    "m = 2\n",
    "T = 120\n",
    "p = 0.5\n",
    "\n",
    "n = T * m  # 수정됨: T.m 대신 T * m 사용\n",
    "\n",
    "rand_points_opt = np.isin(\n",
    "    np.arange(1, T + 1), [1] + [i * m + 1 for i in range(2, int(n) - 1)]\n",
    ")\n",
    "\n",
    "d_opt = gen_d(rand_points_opt, p)\n",
    "y_opt = y_given_d(d_opt, T=T, seed=1)\n",
    "\n",
    "pd.DataFrame(dict(rand_points=rand_points_opt, d=d_opt, delivery_time=y_opt)).to_csv(\n",
    "    \"../data/sb_exp_opt.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T19:59:00.655823Z",
     "start_time": "2023-01-24T19:59:00.603890Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "\n",
    "# confounders\n",
    "age = 18 + np.random.normal(24, 4, n).round(1)\n",
    "income = 500 + np.random.gamma(1, age * 100, n).round(0)\n",
    "credit_score = (np.random.beta(1, 3, n) * 1000).round(0)\n",
    "\n",
    "\n",
    "u = np.random.uniform(-1, 1, n)\n",
    "\n",
    "categs = [\"never-taker\", \"complier\"]\n",
    "\n",
    "cutoff = 0.6\n",
    "e = 1 / (1 + np.exp(-(u - 0.05 * age + 0.01 * credit_score)))\n",
    "cust_categ = np.select([e <= cutoff, e > cutoff], categs)\n",
    "\n",
    "# plt.hist(e)\n",
    "\n",
    "# Instrument\n",
    "prime_elegible = np.random.binomial(1, 0.5, n)\n",
    "choose_prime = ((cust_categ == \"complier\") & (prime_elegible == 1)).astype(int)\n",
    "\n",
    "# outcome\n",
    "group_effect = np.select(\n",
    "    [cust_categ == \"complier\", cust_categ == \"never-taker\"], [700, 200]\n",
    ")\n",
    "\n",
    "pv_0 = np.random.normal(200 + 0.4 * income + 10 * age - 500 * u, 200).round(2)\n",
    "pv_1 = pv_0 + group_effect\n",
    "\n",
    "tau = pv_1 - pv_0\n",
    "pv = pv_0 + group_effect * choose_prime\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        categ=cust_categ,\n",
    "        age=age,\n",
    "        income=income,\n",
    "        credit_score=credit_score,\n",
    "        prime_elegible=prime_elegible,\n",
    "        prime_card=choose_prime,\n",
    "        pv=pv,\n",
    "        tau=tau,\n",
    "    )\n",
    ")[\n",
    "    [\n",
    "        \"age\",\n",
    "        \"income\",\n",
    "        \"credit_score\",\n",
    "        \"prime_elegible\",\n",
    "        \"prime_card\",\n",
    "        \"pv\",\n",
    "        \"tau\",\n",
    "        \"categ\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df.to_csv(\"../data/prime_card.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
